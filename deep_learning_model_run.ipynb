{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOyBrY7C3xCo"
      },
      "source": [
        "### Mounting Drive from Colab\n",
        "\n",
        "Google Drive space to store data and scripts:\n",
        "\n",
        "- folder where results are written (e.g. projects/deep_learning_for_breeding/iteration1/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONzNe74RyzAa"
      },
      "outputs": [],
      "source": [
        "#this cell mounts the user's google drive in the specified folder,\n",
        "#but only once (doing more than once would generate an error)\n",
        "import os\n",
        "\n",
        "gdrive_folder = '/content/gdrive'\n",
        "project_folder = '/content/gdrive/MyDrive/projects/deep_learning_for_breeding/temp' ## !! IMPORTANT: change this depending on data iteration !!\n",
        "\n",
        "if not os.path.isdir(gdrive_folder):\n",
        "  from google.colab import drive\n",
        "  drive.mount(gdrive_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVSr65Mh31R2"
      },
      "source": [
        "### Importing data\n",
        "\n",
        "- select file name (e.g. phenotypes_iteration1) (default: 'phenotypes')\n",
        "- select trait: e.g. 'simphe_mean0_hSquare0.7_cv0.1_QTN1000_A100_D0_AA0_AD0_DA0_DD0_epoch1649239502'\n",
        "\n",
        "kinship matrices are hosted on [zenodo.org](https://zenodo.org/record/6602439#.YpofCHVBxhE);\n",
        "simulated phenotypes are hosted on **Github** (https://github.com/filippob/paper_deep_learning_vs_gblup)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7JaMg2a3wC3"
      },
      "outputs": [],
      "source": [
        "#for data import scripts\n",
        "import argparse\n",
        "\n",
        "#general import used everywhere\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "## file name (default 'phenotypes')\n",
        "fname = 'phenotypes_iteration1'\n",
        "\n",
        "## parameters\n",
        "### iteration 1\n",
        "trait='simphe_mean0_hSquare0.7_cv0.1_QTN1000_A100_D0_AA0_AD0_DA0_DD0_epoch1649239502'\n",
        "#trait='simphe_mean0_hSquare0.7_cv0.1_QTN1000_A75_D25_AA0_AD0_DA0_DD0_epoch1649239502'\n",
        "#trait='simphe_mean0_hSquare0.7_cv0.1_QTN1000_A50_D50_AA0_AD0_DA0_DD0_epoch1649240345'\n",
        "#trait='simphe_mean0_hSquare0.7_cv0.1_QTN1000_A25_D75_AA0_AD0_DA0_DD0_epoch1649240345'\n",
        "#trait='simphe_mean0_hSquare0.7_cv0.1_QTN1000_A0_D100_AA0_AD0_DA0_DD0_epoch1649240989'\n",
        "#trait='simphe_mean0_hSquare0.7_cv0.1_QTN1000_A33_D33_AA34_AD0_DA0_DD0_epoch1649241541'\n",
        "#trait='simphe_mean0_hSquare0.7_cv0.1_QTN1000_A33_D33_AA0_AD34_DA0_DD0_epoch1649241541'\n",
        "#trait='simphe_mean0_hSquare0.7_cv0.1_QTN1000_A33_D33_AA0_AD0_DA34_DD0_epoch1649242513'\n",
        "#trait='simphe_mean0_hSquare0.7_cv0.1_QTN1000_A33_D33_AA0_AD0_DA0_DD34_epoch1649242513'\n",
        "#trait='simphe_mean0_hSquare0.7_cv0.1_QTN1000_A33_D33_AA8.5_AD8.5_DA8.5_DD8.5_epoch1649246514'\n",
        "\n",
        "#loading import functions\n",
        "!wget -O import_kinship.py https://raw.githubusercontent.com/filippob/paper_deep_learning_vs_gblup/main/support_scripts/import_kinship.py\n",
        "!wget -O import_phenotype.py https://raw.githubusercontent.com/filippob/paper_deep_learning_vs_gblup/main/support_scripts/import_phenotype.py\n",
        "!wget -O import_functions.py https://raw.githubusercontent.com/filippob/paper_deep_learning_vs_gblup/main/support_scripts/import_functions.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#should we download the data? (if it's already there, there's no need to\n",
        "#waste time, this is useful for reruns of the script without resetting the env)\n",
        "if 'kinship' not in globals():\n",
        "  #importing cattle data in two variables: kiship and phenotypes\n",
        "  %run import_kinship.py -r https://zenodo.org/record/6602439/files/ -s /content/data/ -d cattle\n",
        "\n",
        "  #transposing kinship so that it's feeedable to keras\n",
        "  kinship = np.transpose(kinship, (2,1,0))\n",
        "else:\n",
        "  print('Kinship data already saved locally, skipping download')\n",
        "\n",
        "## import phenotypic data\n",
        "%run import_phenotype.py -r https://raw.githubusercontent.com/filippob/paper_deep_learning_vs_gblup/main/data/simulated_phenotypes/ -s /content/data/ -d cattle --fname \"$fname\" -p \"$trait\""
      ],
      "metadata": {
        "id": "WvYn431f1_81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4ABqgKS5UaJ"
      },
      "source": [
        "### Data preprocessing - phenotypes\n",
        "\n",
        "Phenotypic data are standardized: mean 0 and standard deviation 1 (as for GBLUP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRXej5sX5WqA"
      },
      "outputs": [],
      "source": [
        "#normalization\n",
        "phenotypes_mean = np.mean(phenotypes)\n",
        "phenotypes_sd = np.std(phenotypes)\n",
        "phenotypes = (phenotypes - phenotypes_mean) / phenotypes_sd\n",
        "\n",
        "#taking a look at the target variable\n",
        "from matplotlib import pyplot as plt\n",
        "plt.hist(phenotypes, bins = 15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mF9HMpQV5g_6"
      },
      "source": [
        "## Build the neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dmlFn9Q5kMr"
      },
      "source": [
        "### Import support functions\n",
        "\n",
        "Download support functions to:\n",
        "\n",
        "- set up the deep learning model\n",
        "- apply data augmentation if needed\n",
        "- calculate performance metrics\n",
        "- save results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vcrm-iG5i9B"
      },
      "outputs": [],
      "source": [
        "#keras custom metrics, relevant functions: pearson() and rmse()\n",
        "!wget -O keras_metrics.py https://raw.githubusercontent.com/filippob/paper_deep_learning_vs_gblup/main/support_scripts/keras_metrics.py\n",
        "%run keras_metrics.py\n",
        "\n",
        "#general toolbox\n",
        "!wget -O keras_toolbox.py https://raw.githubusercontent.com/filippob/paper_deep_learning_vs_gblup/main/support_scripts/keras_toolbox.py\n",
        "%run keras_toolbox.py\n",
        "\n",
        "#functions to parse and save results\n",
        "!wget -O save_results.py https://raw.githubusercontent.com/filippob/paper_deep_learning_vs_gblup/main/support_scripts/save_results.py\n",
        "%run save_results.py\n",
        "\n",
        "#functions to aid data augmentation\n",
        "!wget -O data_augmentation_toolbox.py https://raw.githubusercontent.com/filippob/paper_deep_learning_vs_gblup/main/support_scripts/data_augmentation_toolbox.py\n",
        "%run data_augmentation_toolbox.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBnbLtGMQnQN"
      },
      "source": [
        "### Model parameters\n",
        "\n",
        "Define parameters to run the deep learning model (run at least 20 epochs to be able to save results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHuj_pRGQqBE"
      },
      "outputs": [],
      "source": [
        "#dictionary\n",
        "config = dict({'input_shape':(kinship.shape[1], kinship.shape[2], 1),\n",
        "               'batch_size':64,\n",
        "               'num_epochs':30,\n",
        "               'val_split':0,\n",
        "               'learn_rate':0.001,\n",
        "               'architecture':'Dense',\n",
        "               'pool_step' : 2,\n",
        "               'drop_rate' : 0.25,\n",
        "               'regularizer_l1' : None,\n",
        "               'regularizer_l2' : None\n",
        "               })\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sbgt2XqS_DDh"
      },
      "source": [
        "### Resampling data and fitting the NN model\n",
        "\n",
        "Training - Validation split: 80% - 20%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYTR5YnJ-o-q"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Input\n",
        "from keras.losses import MeanSquaredError, CosineSimilarity\n",
        "import tensorflow.keras.backend as K\n",
        "import json\n",
        "from textwrap import wrap\n",
        "import datetime;\n",
        "\n",
        "#comment/uncomment/create an experiment configuration block, below\n",
        "\n",
        "#------------------------\n",
        "#------------------------\n",
        "#experiment: best five layers, data augm on x\n",
        "config['experiment'] = '5L[32, 64][64, 32, 16], gaussian augmentation on X'\n",
        "config['experiment_desc'] = ''\n",
        "config['conv_layers'] = [32, 64]\n",
        "config['dense_layers'] = [64, 32, 16]\n",
        "reps_values    = [1]                    # n. of augmented copies of the data\n",
        "sigma_X_values = [0.1]                  # magnitude of noise added to augmented data\n",
        "sigma_y_values = [None]\n",
        "suffix = 'selected_net'\n",
        "save_predictions = False                 # if we want to save predictions to a file\n",
        "\n",
        "## number of replicates\n",
        "for rep in range(1): ## change range(n) for the number of replicates n\n",
        "  for i in range(len(reps_values)):\n",
        "    print(\" - updating the config object\")\n",
        "    config['gaussAugm_reps'] = reps_values[i]\n",
        "    config['gaussAugm_sigma_x'] = sigma_X_values[i]\n",
        "    config['gaussAugm_sigma_y'] = sigma_y_values[i]\n",
        "\n",
        "    print(\" - defining the model\")\n",
        "    model = instantiate_network(config)\n",
        "    model.trainable = True\n",
        "    trainable_count = np.sum([K.count_params(w) for w in model.trainable_weights])\n",
        "\n",
        "    print(\" - compiling the model\")\n",
        "    rmsprop = tf.keras.optimizers.RMSprop(learning_rate=config['learn_rate'])\n",
        "    model.compile(loss = MeanSquaredError(), optimizer = rmsprop, metrics = [pearson, rmse])\n",
        "\n",
        "    print(\" - splitting train and validation sets\")\n",
        "    (train_x, train_y, val_x, val_y, sel_val) = train_val_split(x=kinship, y=phenotypes, validation_split=0.2)\n",
        "\n",
        "    print(\" - augmenting train data with gaussian noise\")\n",
        "    (train_x_augm, train_y_augm) = augment_add_normal_noise(x = train_x, y = train_y,\n",
        "                                                            reps=config['gaussAugm_reps'],\n",
        "                                                            mu = None,     #mu of noise on y\n",
        "                                                            sigma = config['gaussAugm_sigma_y'],  #var of noise on y\n",
        "                                                            mu_x    = 0,   #mu of noise on x\n",
        "                                                            sigma_x = config['gaussAugm_sigma_x']) #var of noise on x\n",
        "\n",
        "    #room for training history\n",
        "    h = None\n",
        "\n",
        "    print(\" - fitting the model\")\n",
        "    for epochs in range(0, config['num_epochs'], 10):\n",
        "      print('   - doing epochs ' + str(epochs) + ' -> ' + str(epochs + 10))\n",
        "\n",
        "      #train for 10 epochs on training set, agumented\n",
        "      h_train = model.fit(train_x_augm, train_y_augm,\n",
        "          batch_size=config['batch_size'],\n",
        "          epochs = 10,\n",
        "          validation_split = 0,\n",
        "          verbose=1)\n",
        "\n",
        "      #measuring perfomances on the validation set\n",
        "      validation_metrics = model.evaluate(val_x, val_y, batch_size=config['batch_size'])\n",
        "\n",
        "      #keeping track\n",
        "      h = merge_history(\n",
        "          train_set_history = h_train,\n",
        "          val_set_evaluation = validation_metrics,\n",
        "          metrics = model.metrics_names,\n",
        "          past_merged_history = h)\n",
        "\n",
        "    #training is over, let's just update the internal epochs count\n",
        "    h.params['epochs'] = epochs + 10\n",
        "          ## saving predictions if required\n",
        "\n",
        "    ## the model object is used to extract predictions\n",
        "    ## if not reinstantiated, I believe that the model object keeps being over\n",
        "    ## epochs in the above for loop; therefore the model object after the loop\n",
        "    ## containes the fully trained model to be used for predictions\n",
        "    if save_predictions == True:\n",
        "      print(\" - saving predictions\")\n",
        "      predictions = get_predictions(model, val_x, val_y, sel_val, config)\n",
        "      fname = os.path.join(project_folder, trait, suffix, \"predictions.csv\")\n",
        "      print(\"writing results to: \", fname)\n",
        "      writeout_results(predictions, fname)\n",
        "      ## calculating NDCG on predictions (val set)\n",
        "      ndcg10  = ndcg(predictions['y'], predictions['y_hat'], 0.10)\n",
        "      ndcg20  = ndcg(predictions['y'], predictions['y_hat'], 0.20)\n",
        "      ndcg50  = ndcg(predictions['y'], predictions['y_hat'], 0.50)\n",
        "      ndcg100 = ndcg(predictions['y'], predictions['y_hat'], 1.0)\n",
        "\n",
        "    print(\" - evaluating results\")\n",
        "    timestamp = datetime.datetime.now()\n",
        "    #config['timestamp'] = str(timestamp) # commented out because get_predictions() already produces a timestamp column in config\n",
        "    max_val_pearson = str(np.nanmax(h.history['val_pearson']))\n",
        "    print(\"Best value for correlation on VAL set: \" + max_val_pearson)\n",
        "    res = parse_history(h, phenotypes, trait, config, max_val_pearson, trainable_count, replicate = rep)\n",
        "    res['ndcg10']  = ndcg10  if 'ndcg10'  in globals() else NaN\n",
        "    res['ndcg20']  = ndcg20  if 'ndcg20'  in globals() else NaN\n",
        "    res['ndcg50']  = ndcg50  if 'ndcg50'  in globals() else NaN\n",
        "    res['ndcg100'] = ndcg100 if 'ndcg100' in globals() else NaN\n",
        "    display(res)\n",
        "\n",
        "    print(\" - writing out results\")\n",
        "    fname = os.path.join(project_folder, trait, suffix, \"results.csv\")\n",
        "    print(\"writing results to: \", fname)\n",
        "    writeout_results(res, fname)\n",
        "\n",
        "    #save plots\n",
        "    if np.random.randn() > 1.5:\n",
        "      blob = '\\n'.join(wrap(json.dumps(config),60))\n",
        "      fname = os.path.join(os.path.dirname(fname), str(timestamp) + \"_loss.png\")\n",
        "      plot_loss_history(h, 'loss', fname, blob)\n",
        "      fname = os.path.join(os.path.dirname(fname), str(timestamp) + \"_pearson.png\")\n",
        "      plot_loss_history(h, 'pearson', fname, blob)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEGpoJDV4Jsy"
      },
      "outputs": [],
      "source": [
        "plot_loss_history(h, 'pearson')\n",
        "print(\"Best value for correlation on VAL set: \" + str(np.max(h.history['val_pearson'])))\n",
        "print(\"Mean correlation on last five epochs on VAL set: \" + str(np.average(h.history['val_pearson'][-5:])))\n",
        "\n",
        "plot_loss_history(h, 'loss')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}